---
title: O
permalink: /o/

#=== THEMES
# = minima
layout: page
# = minimal mistake
# layout: archive-taxonomy
# layout: archive
# layout: categories
# layout: category
# layout: collection
# layout: compress
# layout: default
# layout: home
# layout: posts
# layout: search
## layout: single
# layout: splash
# layout: tag
# layout: tags

---

* toc
{:toc}

{% include links/all.md %}


# Object Detection

 ![]( {{site.assets}}/o/object_detection.png ){: width="100%"}

 More at:
  * [https://heartbeat.fritz.ai/the-5-computer-vision-techniques-that-will-change-how-you-see-the-world-1ee19334354b](https://heartbeat.fritz.ai/the-5-computer-vision-techniques-that-will-change-how-you-see-the-world-1ee19334354b)
  * [https://docs.ultralytics.com/tasks/detect/](https://docs.ultralytics.com/tasks/detect/)

 See also [O], [Computer Vision], [Convolutional Neural Network], [Image Segmentation]


# Object Recognition

 See also [O], [Computer Vision]


# Object Tracking

 More at:
  * [https://heartbeat.fritz.ai/the-5-computer-vision-techniques-that-will-change-how-you-see-the-world-1ee19334354b](https://heartbeat.fritz.ai/the-5-computer-vision-techniques-that-will-change-how-you-see-the-world-1ee19334354b)

 See also [O], [Computer Vision]


# Objective Function

 The "objective function" is the function that you want to minimize or maximize in your problem.
  * If minimize, the objective function is also called a loss, error, or fitness function, but more often a [cost function]
  * If maximize, the objective function is called a maximization or maximize function

 The expression "objective function" is used in several different contexts (e.g. [machine learning] or [linear programming]), but it always refers to the function to be maximized or minimized in the specific (optimization) problem. Hence, this expression is used in the context of mathematical optimization.

 For example, in machine learning, you define a model, M. To train M, you usually define a loss function L (e.g., a [mean squared error]), which you want to minimize. L is the "objective function" of your problem (which in this case is to be minimized).

 In the context of [search algorithms], the objective function could represent e.g. the cost of the solution. For example, in the case of the [traveling salesman problem (TSP)][TSP], you define a function, call it C , which represents the "cost" of the tour or [Hamiltonian cycle], that is, a function which sums up the weights of all edges in the tour. In this case, the "objective" of your problem is to minimize this function C , because, essentially, you want to find an inexpensive tour, which is associated with either a local (or global) minimum of C . This function C is the "objective function".

 It should now be easy to memorize the expression "objective function", as it contains the term "objective", and the "objective" (or goal) in your ([optimization]) problem is to minimize (or maximize) the corresponding function.

 ![]( {{site.assets}}/o/objective_function.jpeg ){: width="100%"}

 More at:
  * [https://ai.stackexchange.com/questions/9005/what-is-an-objective-function](https://ai.stackexchange.com/questions/9005/what-is-an-objective-function)

 See also [O], ...


# Observation

 In Reinforcement learning, an observation using one or more sensor (ex: camera) can help you identify in which state you are.

 From the observation, you try to define in which state you are!

 ![]( {{site.assets}}/o/observation_deepracer.png ){: width="100%"}

 See also [O], [Reinforcement Learning], [State]


# Off-Policy Learning

 A very common scenario for off-policy learning is to learn about best guess at optimal policy from an exploring policy, but that is not the definition of off-policy.  The primary difference between observations generated by ùëè and the target policy ùúã is which actions are selected on each time step. There is also a secondary difference which can be important: The population distribution of both states and actions in the observations can be different between ùëè and ùúã - this can have an impact for function approximation, as cost functions (for e.g. NNs) are usually optimised over a population of data.

 More at:
  * [https://ai.stackexchange.com/questions/10474/what-is-the-relation-between-online-or-offline-learning-and-on-policy-or-off](https://ai.stackexchange.com/questions/10474/what-is-the-relation-between-online-or-offline-learning-and-on-policy-or-off)

 See also [O], ...


# Off-Policy Learning Algorithm

 Those algorithms are specific to [control systems] and [Reinforcement Learning]. Despite the similarities in name between these concepts and [online learning] / [offline learning], they refer to a different part of the problem.

 Off-policy algorithms work with two policies (sometimes effectively more, though never more than two per step). These are a policy being learned, called the target policy (usually shown as ùúã), and the policy being followed that generates the observations, called the behaviour policy (called various things in the literature - ùúá , ùõΩ , Sutton and Barto call it ùëè in the latest edition).

 Examples of off-policy learning algorithms:
 * [Deep Q-Network][DQN]
 * [Deep Deterministic Policy Gradient (DDPG)][DDPG]
 * [Twin Delayed Deep Deterministic Policy Gradient (TD3)][TD3]
 * [Soft Actor-Critic (SAC)][SAC]

 ![]( {{site.assets}}/o/off_policy_learning_algorithm.png ){: width="100%"}

 More at:
  * [https://ai.stackexchange.com/questions/10474/what-is-the-relation-between-online-or-offline-learning-and-on-policy-or-off](https://ai.stackexchange.com/questions/10474/what-is-the-relation-between-online-or-offline-learning-and-on-policy-or-off)

 See also [O], ...


# Offline Learning

 ~ learning from stored and aging data!

 Offline and [online learning] concepts are not specific to [reinforcement learning] and different from [Off-policy learning] / [on-policy learning], many learning systems can be categorised as online or offline (or somewhere in-between).

 Offline learning algorithms work with data in bulk, from a dataset. Strictly offline learning algorithms need to be re-run from scratch in order to learn from changed data. Support vector machines and random forests are strictly offline algorithms (although researchers have constructed online variants of them).

 Offline learning algorithms aim to extract knowledge and patterns from the provided dataset to build a model or policy that can make decisions or take actions in similar situations. These algorithms often employ techniques like supervised learning, inverse reinforcement learning, or model-based RL to learn from the offline data.

 It's important to note that offline learning can introduce certain challenges, such as dataset biases, distributional shift between the dataset and the deployment environment, and the risk of learning from suboptimal or noisy data. Addressing these challenges is an active area of research in offline RL.

 More at:
  * [https://ai.stackexchange.com/questions/10474/what-is-the-relation-between-online-or-offline-learning-and-on-policy-or-off](https://ai.stackexchange.com/questions/10474/what-is-the-relation-between-online-or-offline-learning-and-on-policy-or-off)

 See also [O], ...


# Offline Learning Algorithm

 Algorithm that can do [offline learning].

 See also [O], ...


# Omniverse

 The metaverse by [Nvidia]

 More at:
  * [https://venturebeat.com/ai/architects-and-engineers-find-a-transformative-solution-in-nvidia-omniverse/](https://venturebeat.com/ai/architects-and-engineers-find-a-transformative-solution-in-nvidia-omniverse/)

 See also [0], ...


# On-Policy Learning

 On-policy learning is a type of reinforcement learning (RL) algorithm where an agent learns by directly interacting with an environment and using the data collected during the learning process. In on-policy learning, the agent follows a specific policy, which is a strategy that determines its actions based on the observed states.

 During the learning process, the agent collects experiences by taking actions in the environment and receiving feedback in the form of rewards or penalties. It uses these experiences to update its policy iteratively, typically through methods like policy gradients or temporal difference learning.

 The key characteristic of on-policy learning is that the data used for updating the policy comes from the current policy's own interactions with the environment. As a result, the policy being learned affects the trajectory of the agent, and any changes to the policy will impact the exploration and exploitation of the environment.

 One advantage of on-policy learning is that it can effectively handle changing or non-stationary environments since the agent continually interacts with the current environment state. However, on-policy learning algorithms can be more sample-inefficient compared to off-policy algorithms, as they require new data from the environment for each policy update.

 See also [O], ...


# On-Policy Learning Algorithm

 An algorithm that does [on-policy learning] !

 Examples of on-policy learning algorithms include 
  * [REINFORCE (Monte Carlo Policy Gradient)][REINFORCE], 
  * [Asynchronous Advantage Actor-Critic (A3C)][A3C]
  * [Proximal Policy Optimization (PPO)][PPO], and
  * [Trust Region Policy Optimization (TRPO)][TRPO].

 ![]( {{site.assets}}/o/on_policy_learning_algorithm.png ){: width="100%"}

 See also [O], ...


# One-Armed Bandit

  * choice between slot machines (one-armed bandit)
  * average profitability NOT known in advance
  * Learning problem = ignorance of profitability

 See also [O], ...


# One-Cold Encoding

 Same as [one-hot encoding] except the 1 is a 0 and 0s are 1s. Example yellow = [1, 0, 1, 1].

 See also [O], [Encoding], [One-Hot Encoding], [Ordinal Encoding]


# One-Hot Encoding

 [Categorical features][Categorical Feature] often need to be converted to numeric values to facilitate processing by modeling algorithms.

 Categorical data refers to variables that are made up of label values, for example, a ‚Äúcolor‚Äù variable could have the values ‚Äúred‚Äú, ‚Äúblue, and ‚Äúgreen‚Äù. Think of values like different categories that sometimes have a natural ordering to them. Some machine learning algorithms can work directly with categorical data depending on implementation, such as a decision tree, but most require any inputs or outputs variables to be a number, or numeric in value. This means that any categorical data must be mapped to integers. One hot encoding is one method of converting data to prepare it for an algorithm and get a better prediction. With one-hot, we convert each categorical value into a new categorical column and assign a binary value of 1 or 0 to those columns. Each integer value is represented as a binary vector. All the values are zero, and the index is marked with a 1.
 
 ```
red    = [1, 0, 0, 0]           <== vector !
yellow = [0, 1, 0, 0]
blue   = [0, 0, 1, 0]
green  = [0, 0, 0, 1]
 ```

 with pandas
 
 ```
import pandas as pd

df = pd.DataFrame({"col1": ["Sun", "Sun", "Moon", "Earth", "Moon", "Venus"]})
df_new = pd.get_dummies(df, columns=["col1"], prefix="Planet")
print(df)

#     Planet_Earth  Planet_Moon  Planet_Sun  Planet_Venus
# 0             0            0           1             0                <== Sun
# 1             0            0           1             0                <== Sun
# 2             0            1           0             0                <== Moon
# 3             1            0           0             0                <== Earth
# 4             0            1           0             0                <== Moon
# 5             0            0           0             1                <== Venus
 ```

 See also [O], [Encoding], [One-Cold Encoding], [Ordinal Encoding]


# One-Shot Learning

 A [prompt engineering] technique to increase the accuracy of a [large language model].

 In addition to the task description, the model sees a single example of the task. No gradient updates are performed.

 ```
Translate English to French:                # Task description
sea otter => loutre de mer                  # Example
cheese =>                                   # Prompt
 ```

 ~ `not enough data --> encoder + similarity function?` Ex: I only have 1 picture of the employee, how can I recognize people coming in as an employee or not? Treat as a classification problem and each class correspond to 1 employee. How do you learn to train a classifier based on 1 sample? Better to look at this problem as a nearest-neighbor problem! Question: How do you detect similarities? Use a similarity function, an encoder! So sequence of (1) an encoder and (2) a nearest neighbor algorithm.

 ![]( {{site.assets}}/o/one_shot_learning.png ){: width="100%"}

 Few-, one-, and zero-shot settings are specialized cases of zero-shot task transfer. In a few-shot setting, the model is provided with a task description and as many examples as fit into the context window of the model. In a one-shot setting, the model is provided with exactly one example and, in a zero-shot setting, with no example.

 ![]( {{site.assets}}/o/one_shot_learning_accuracy.png ){: width="100%"}

 More at:
  * [https://en.wikipedia.org/wiki/One-shot_learning_(computer_vision)](https://en.wikipedia.org/wiki/One-shot_learning_(computer_vision))

 See also [O], [Encoder], [Few-Shot Learning], [Siamese Network], [Similarity Function], [Transfer Learning], [Zero-Shot Learning], [Zero-Shot Task Transfer]


# Online Learning

 ~ Learning from streamed data!

 In computer science, online learning is a method of machine learning in which data becomes available in a sequential order and is used to update the best predictor for future data at each step, as opposed to batch learning techniques which generate the best predictor by learning on the entire training data set at once. Online learning is a common technique used in areas of machine learning where it is computationally infeasible to train over the entire dataset, requiring the need of out-of-core algorithms. It is also used in situations where it is necessary for the algorithm to dynamically adapt to new patterns in the data, or when the data itself is generated as a function of time, e.g., stock price prediction. Online learning algorithms may be prone to catastrophic interference, a problem that can be addressed by incremental learning approaches.

 More at:
  * [https://ai.stackexchange.com/questions/10474/what-is-the-relation-between-online-or-offline-learning-and-on-policy-or-off](https://ai.stackexchange.com/questions/10474/what-is-the-relation-between-online-or-offline-learning-and-on-policy-or-off)
  * [https://en.wikipedia.org/wiki/Online_machine_learning](https://en.wikipedia.org/wiki/Online_machine_learning)

 See also [O], [Buffered Online Learning], [Offline Learning]


# Online Learning Algorithm

 An algorithm that supports [online learning].

 See also [O], ....


# Open Neural Network Exchange

# ONNX

 A common format to store a model?

 More at:
  * [https://onnx.ai/](https://onnx.ai/)

 See also [O], [OpenVINO Toolkit]


# OpenAI Company

 [Microsoft] + [OpenAI] ~ [Google] + [DeepMind]

 Models:
  * [ChatGPT][ChatGPT Model]: An fine-tuned model of GPT that is based on dialog
  * [CLIP][CLIP Model]: A model that can put a legend to an image
  * [Codex][Codex Model]: A [LLM] that specialize on generating code
  * [DALL-E][DALL-E Model]: A [Diffusion Model] that from text can generate images
  * [Five][OpenAI Five Model]: An agent that isnow world champion at the Dota2 game!
  * [GPT][GPT Model]: A Generative model for text
  * [Gym Environment][OpenAI Gym Environment]: Environments for development of [Reinforcement Learning] algorithms.
  * [Jukebox][Jukebox Model]: Generative model for music
  * [Whisper][Whisper Model]: A speech-to-text model

 Interfaces
  * [Triton] : GPU programming for neural networks

 People:
  * [Sam Altman][Sam Altman Person]: CEO
  * [Greg Brockman][Greg Brockman Person]: Co-founder
  * [Ilya Sutskever][Ilya Sutskever Person]: Co-founder
  * [Elon Mustk][Elon Musk Person]: Early investor
  * [Bill Gates][Bill Gates Person]: Early/Late investor

 More at:
  * home - [http://www.openai.com](http://www.openai.com)
  * principles - [https://openai.com/policies/usage-policies](https://openai.com/policies/usage-policies)
  * safety - [https://openai.com/safety-standards](https://openai.com/safety-standards)

 See also [O], [People]


# OpenAI Five Model

 September 2018
 5 agents (characters) that work together in collaboration! 
 In a game with incomplete information

 {% youtube "https://www.youtube.com/watch?v=tfb6aEUMC04" %}

 More at:
  * [https://openai.com/research/openai-five](https://openai.com/research/openai-five)
  * [https://openai.com/blog/openai-five-benchmark](https://openai.com/blog/openai-five-benchmark)
  * [https://www.twitch.tv/videos/293517383](https://www.twitch.tv/videos/293517383)
  * [https://openai.com/blog/openai-five-finals](https://openai.com/blog/openai-five-finals)
  * wikipedia - [https://en.wikipedia.org/wiki/OpenAI_Five](https://en.wikipedia.org/wiki/OpenAI_Five)

 See also [O], [AlphaStar Model]


# OpenAI Function

 {% youtube "https://www.youtube.com/watch?v=0gPh18vRghQ" %}

 More at:
  * docs - [https://openai-functions.readthedocs.io/en/latest/introduction.html](https://openai-functions.readthedocs.io/en/latest/introduction.html)
  * [https://openai.com/blog/function-calling-and-other-api-updates?ref=upstract.com](https://openai.com/blog/function-calling-and-other-api-updates?ref=upstract.com)
  * [https://towardsdatascience.com/an-introduction-to-openai-function-calling-e47e7cd7680e](https://towardsdatascience.com/an-introduction-to-openai-function-calling-e47e7cd7680e)

 See also [O], ...


# OpenAI Gym Environment

 Virtual environment built by [OpenAI] and to be used for [Reinforcement Learning] projects

 See also [O], [Rocket League Gym]


# OpenCV Module

 More at:
  * Sample project - [https://github.com/ecemay/open_cv_studies](https://github.com/ecemay/open_cv_studies)

 See also [O], [Computer Vision]


# OpenFold Model

 OpenFold is a non-profit AI research and development consortium developing free and open-source software tools for biology and drug discovery. Our mission is to bring the most powerful software ever created -- AI systems with the ability to engineer the molecules of life -- to everyone. These tools can be used by academics, biotech and pharmaceutical companies, or students learning to create the medicines of tomorrow, to accelerate basic biological research, and bring new cures to market that would be impossible to discover without AI.

 More at:
  * [https://openfold.io/](https://openfold.io/)

 See also [O], [AlphaFold Model], [ESM Metagenomic Atlas]


# OpenMMLab

 OpenMMLab team integrates open source, academic research, business application and system development, aiming at becoming a worldwide leader in open source algorithm platform for computer vision and providing advanced R&D models and systems for the industry.

 Based on [PyTorch]

 More at:
  * [https://openmmlab.com/](https://openmmlab.com/)

 See also [O], ...


# OpenVINO Toolkit
 
 More at:
  * home - [https://docs.openvino.ai/latest/home.html](https://docs.openvino.ai/latest/home.html)

 See also [O], [Open Neural Network Exchange]


# Optimal Policy

 In [reinforcement learning], an optimal policy refers to the strategy that maximizes expected [cumulative reward] for an [RL agent].

  * A policy defines how an agent takes actions in a particular state of the environment to maximize rewards. It maps states to actions.
  * The optimal policy is the one that results in the highest total reward over time across all states.
  * It maximizes the expected return - the sum of discounted future rewards - from any initial state.
  * For simple environments, optimal policies can sometimes be derived mathematically using dynamic programming.
  * But in complex environments, machine learning is used to learn good policies through trial-and-error experience.
  * Reinforcement learning algorithms like Q-learning, policy gradients, and deep Q-networks aim to learn the optimal policy that maximizes long-term reward.
  * However, they may converge to near-optimal policies only, especially in environments with partial observability.
  * The learned policy is considered optimal if no other policy results in significantly higher reward over multiple trials.
  * Evaluation metrics like cumulative reward, win rate, and score determine how close a learned policy is to the theoretical optimal.
  * The optimal policy balances short-term and long-term rewards. It exploits known rewards while still exploring for better returns.

 So in summary, the optimal policy maximizes overall reward from the environment through an ideal strategy for taking actions given particular state information. RL agents attempt to learn policies that approach optimal returns.

 See also [O], ...


# Optimal Q-Value

 This value is the maximum value in the [Q-table] for a given state. The Optimal [Q-valuei] helps the [agent] decides what action to take in the case of [exploitation].

 In Q-learning and other reinforcement learning algorithms, the optimal Q-value for a given state-action pair refers to the maximum expected future reward that can be obtained by taking that action in that state. Here are some key points:
  * Q-values represent the quality or desirability of taking a given action in a particular state based on expected future rewards.
  * The optimal Q-value is the highest possible Q-value for that state-action pair. It quantifies the maximum future reward the agent can achieve by taking that action.
  * Finding the optimal policy involves learning the optimal Q-values for each state-action pair. The optimal action for a state has the highest Q-value.
  * In tabular Q-learning, the optimal Q-values can be learned iteratively by updating a Q-table through experience: Q(s,a) += alpha * (R + gamma * max Q(s',a') - Q(s,a))
  * In deep Q-learning, a neural network approximates the optimal Q-function mapping states and actions to expected returns.
  * The optimal Q-values satisfy the Bellman optimality equation, allowing Q-values to be optimized recursively.
  * Early in training, Q-values may be far from optimal. But through continual updates guided by rewards and the discount factor, they gradually converge closer to the optimal values.
  * The difference between the current Q-value and optimal Q-value for a state-action determines how much more the agent has left to learn about that state-action.
  * Learning is considered complete when Q-values change minimally during updates, indicating they have converged close to optimal.

 So in summary, the optimal Q-value quantifies the maximum future reward for taking a particular action in a given state, guiding optimal action selection.

 See also [O], ...


# Optimization

 See also [O], [Dynamic Programming], [Loss Function], [Optimizer]


# Optimizer

 Optimizer = How you are going to minimize the [Loss Function].

 To minimize the prediction error or loss, the model while experiencing the examples of the training set, updates the model parameters W. These error calculations when plotted against the W is also called cost function plot J(w), since it determines the cost/penalty of the model. So minimizing the error is also called as minimization the cost function. But how exactly do you do that? Using optimizers.

 Optimization algorithm
  * The first algorithms
   * [Gradient Descent (GD)][GD Algorithm]
   * [Stochastic Gradient Descent (SGD)][SGD Algorithm] - faster convergence than GD
  * Adaptive learning algorithms
   * [Resilient Backpropagation (Rrop)][Rprop Algorithm] (1992)
   * [Adaptive Gradient Algorithm][AdaGrad Algorithm] 
   * [Root Mean Square Propagation (RMSprop)][RMSProp Algorithm] 
  * Momentum algorithms
   * [Gradient Descent with Momentum][GD with Momentum algorithm]
  * Adaptive learning with Momentum algorithms
   * [Adaptive Delta Algorithm][AdaDelta Algorithm] 
   * [Adaptive Momentum Estimation (Adam)][Adam Algorithm]
  * Others
   * Nesterov-accelerated Adaptive Momentum Estimation (Nadam)
   * Nadamax
   * Nesterov
   * MaxaProp

 
 ```
from tensorflow import keras
from tensorflow.keras import layers

model = keras.Sequential()
model.add(layers.Dense(64, kernel_initializer='uniform', input_shape=(10,)))
model.add(layers.Activation('softmax'))

opt = keras.optimizers.Adam(learning_rate=0.01)
model.compile(loss='categorical_crossentropy', optimizer=opt)
 ```

 More at:
  * [https://keras.io/api/optimizers/](https://keras.io/api/optimizers/)

 See also [O], ...


# Ordinal Encoding

 [Categorical features][Categorical Feature] often need to be converted to numeric values to facilitate processing by modeling algorithms.

 Ordinal encoding converts categorical column to numeric values when an ordered relationship exists.

 ![]( {{site.assets}}/o/ordinal_encoding.png )

 See also [O], [On-Cold Encoding], [One-Hot Encoding]


# Outlier

 Used in [Feature Engineering], ...
 THe opposite of [Inlier]

 Q: How to identify an outlier? --> [RANSAC Algorithm]

 See also [O], ...


# Output Layer

 See also [O], [Layer], [Artificial Neural Network]


# Output Perturbation

 We introduce gaussian noise in the output. Easy to implement as treat the model as a blackbox. But if people have access to the model itself, they can bypass noise the "addititor". In general, it is better if the noise is built in the model!

 See also [O], [Differential Privacy]


# Overfitting

 ~ a Model that is overly complex and leads to high [variance] and low [bias] = noise is memorized in model! . In contrast, a program that memorizes the training data by learning an overly-complex model could predict the values of the response variable for the training set accurately, but will fail to predict the value of the response variable for new examples.

 `A well functioning ML algorithm will separate the signal from the noise`.

 If the algorithm is too complex or flexible (e.g. it has too many input features or it‚Äôs not properly regularized), it can end up ‚Äúmemorizing the noise‚Äù instead of finding the signal. This overfit model will then make predictions based on that noise. It will perform unusually well on its training data‚Ä¶ yet very poorly on new, unseen data. `A key challenge with overfitting, and with machine learning in general, is that we can‚Äôt know how well our model will perform on new data until we actually test it`. To address this, we can split our initial [dataset] into separate [training][Training Subset] and [test subsets][Test Subset]. To avoid over-fitting, try [hyperparameter optimization].

 Reasons for overfitting:
  * too few training examples
  * running the training process for too many epochs. Consider [early stopping] ?

 ![]( {{site.assets}}/o/overfitting_fitting_comparison.png ){: width="100%"}

 Beware:
  * Overfitting is responsible for the [membership inference attack]
  * The opposite of overfitting is [underfitting]

 See also [O], [Balanced Fitting], [Bias], [Principal Component Analysis]


# Overtraining

 Overtraining is when a machine learning model can predict training examples with very high accuracy but cannot generalize to new data. This leads to poor performance in the field. Usually, this is a result of too little data or data that is too homogenous.

 See also [O], [Overfitting]
