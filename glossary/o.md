---
title: O
permalink: /o/

#=== THEMES
# = minima
layout: page
# = minimal mistake
# layout: archive-taxonomy
# layout: archive
# layout: categories
# layout: category
# layout: collection
# layout: compress
# layout: default
# layout: home
# layout: posts
# layout: search
## layout: single
# layout: splash
# layout: tag
# layout: tags

---

* toc
{:toc}

{% include links/all.md %}


# Object Detection

 ![]( {{site.assets}}/o/object_detection.png ){: width="100%"}

 More at:
  * [https://heartbeat.fritz.ai/the-5-computer-vision-techniques-that-will-change-how-you-see-the-world-1ee19334354b](https://heartbeat.fritz.ai/the-5-computer-vision-techniques-that-will-change-how-you-see-the-world-1ee19334354b)
  * [https://docs.ultralytics.com/tasks/detect/](https://docs.ultralytics.com/tasks/detect/)

 See also [O], [Computer Vision], [Convolutional Neural Network], [Image Segmentation]


# Object Recognition

 See also [O], [Computer Vision]


# Object Tracking

 More at:
  * [https://heartbeat.fritz.ai/the-5-computer-vision-techniques-that-will-change-how-you-see-the-world-1ee19334354b](https://heartbeat.fritz.ai/the-5-computer-vision-techniques-that-will-change-how-you-see-the-world-1ee19334354b)

 See also [O], [Computer Vision]


# Observation

 In Reinforcement learning, an observation using one or more sensor (ex: camera) can help you identify in which state you are.

 See also [O], [Reinforcement Learning], [State]


# Off-Policy Learning

 A very common scenario for off-policy learning is to learn about best guess at optimal policy from an exploring policy, but that is not the definition of off-policy.  The primary difference between observations generated by ùëè and the target policy ùúã is which actions are selected on each time step. There is also a secondary difference which can be important: The population distribution of both states and actions in the observations can be different between ùëè and ùúã - this can have an impact for function approximation, as cost functions (for e.g. NNs) are usually optimised over a population of data.

 More at:
  * [https://ai.stackexchange.com/questions/10474/what-is-the-relation-between-online-or-offline-learning-and-on-policy-or-off](https://ai.stackexchange.com/questions/10474/what-is-the-relation-between-online-or-offline-learning-and-on-policy-or-off)

 See also [O], ...


# Off-Policy Learning Algorithm

 Those algorithms are specific to [control systems] and [Reinforcement Learning]. Despite the similarities in name between these concepts and [online learning] / [offline learning], they refer to a different part of the problem.

 Off-policy algorithms work with two policies (sometimes effectively more, though never more than two per step). These are a policy being learned, called the target policy (usually shown as ùúã), and the policy being followed that generates the observations, called the behaviour policy (called various things in the literature - ùúá , ùõΩ , Sutton and Barto call it ùëè in the latest edition).

 Examples of off-policy learning algorithms:
 * [Deep Q-Network][DQN]
 * [Deep Deterministic Policy Gradient (DDPG)][DDPG]
 * [Twin Delayed Deep Deterministic Policy Gradient (TD3)][TD3]
 * [Soft Actor-Critic (SAC)][SAC]

 ![]( {{site.assets}}/o/off_policy_learning_algorithm.png ){: width="100%"}

 More at:
  * [https://ai.stackexchange.com/questions/10474/what-is-the-relation-between-online-or-offline-learning-and-on-policy-or-off](https://ai.stackexchange.com/questions/10474/what-is-the-relation-between-online-or-offline-learning-and-on-policy-or-off)

 See also [O], ...


# Offline Learning

 ~ learning from stored and aging data!

 Offline and [online learning] concepts are not specific to [reinforcement learning] and different from [Off-policy learning] / [on-policy learning], many learning systems can be categorised as online or offline (or somewhere in-between).

 Offline learning algorithms work with data in bulk, from a dataset. Strictly offline learning algorithms need to be re-run from scratch in order to learn from changed data. Support vector machines and random forests are strictly offline algorithms (although researchers have constructed online variants of them).

 Offline learning algorithms aim to extract knowledge and patterns from the provided dataset to build a model or policy that can make decisions or take actions in similar situations. These algorithms often employ techniques like supervised learning, inverse reinforcement learning, or model-based RL to learn from the offline data.

 It's important to note that offline learning can introduce certain challenges, such as dataset biases, distributional shift between the dataset and the deployment environment, and the risk of learning from suboptimal or noisy data. Addressing these challenges is an active area of research in offline RL.

 More at:
  * [https://ai.stackexchange.com/questions/10474/what-is-the-relation-between-online-or-offline-learning-and-on-policy-or-off](https://ai.stackexchange.com/questions/10474/what-is-the-relation-between-online-or-offline-learning-and-on-policy-or-off)

 See also [O], ...


# Offline Learning Algorithm

 Algorithm that can do [offline learning].

 See also [O], ...


# Omniverse

 The metaverse by [Nvidia]

 More at:
  * [https://venturebeat.com/ai/architects-and-engineers-find-a-transformative-solution-in-nvidia-omniverse/](https://venturebeat.com/ai/architects-and-engineers-find-a-transformative-solution-in-nvidia-omniverse/)

 See also [0], ...


# On-Policy Learning

 On-policy learning is a type of reinforcement learning (RL) algorithm where an agent learns by directly interacting with an environment and using the data collected during the learning process. In on-policy learning, the agent follows a specific policy, which is a strategy that determines its actions based on the observed states.

 During the learning process, the agent collects experiences by taking actions in the environment and receiving feedback in the form of rewards or penalties. It uses these experiences to update its policy iteratively, typically through methods like policy gradients or temporal difference learning.

 The key characteristic of on-policy learning is that the data used for updating the policy comes from the current policy's own interactions with the environment. As a result, the policy being learned affects the trajectory of the agent, and any changes to the policy will impact the exploration and exploitation of the environment.

 One advantage of on-policy learning is that it can effectively handle changing or non-stationary environments since the agent continually interacts with the current environment state. However, on-policy learning algorithms can be more sample-inefficient compared to off-policy algorithms, as they require new data from the environment for each policy update.

 See also [O], ...


# On-Policy Learning Algorithm

 An algorithm that does [on-policy learning] !

 Examples of on-policy learning algorithms include 
  * [REINFORCE (Monte Carlo Policy Gradient)][REINFORCE], 
  * [Asynchronous Advantage Actor-Critic (A3C)][A3C]
  * [Proximal Policy Optimization (PPO)][PPO], and
  * [Trust Region Policy Optimization (TRPO)][TRPO].

 ![]( {{site.assets}}/o/on_policy_learning_algorithm.png ){: width="100%"}

 See also [O], ...


# One-Armed Bandit

  * choice between slot machines (one-armed bandit)
  * average profitability NOT known in advance
  * Learning problem = ignorance of profitability

 See also [O], ...


# One-Cold Encoding

 Same as [one-hot encoding] except the 1 is a 0 and 0s are 1s. Example yellow = [1, 0, 1, 1].

 See also [O], [Encoding], [One-Hot Encoding], [Ordinal Encoding]


# One-Hot Encoding

 [Categorical features][Categorical Feature] often need to be converted to numeric values to facilitate processing by modeling algorithms.

 Categorical data refers to variables that are made up of label values, for example, a ‚Äúcolor‚Äù variable could have the values ‚Äúred‚Äú, ‚Äúblue, and ‚Äúgreen‚Äù. Think of values like different categories that sometimes have a natural ordering to them. Some machine learning algorithms can work directly with categorical data depending on implementation, such as a decision tree, but most require any inputs or outputs variables to be a number, or numeric in value. This means that any categorical data must be mapped to integers. One hot encoding is one method of converting data to prepare it for an algorithm and get a better prediction. With one-hot, we convert each categorical value into a new categorical column and assign a binary value of 1 or 0 to those columns. Each integer value is represented as a binary vector. All the values are zero, and the index is marked with a 1.
 
 ```
red    = [1, 0, 0, 0]           <== vector !
yellow = [0, 1, 0, 0]
blue   = [0, 0, 1, 0]
green  = [0, 0, 0, 1]
 ```

 with pandas
 
 ```
import pandas as pd

df = pd.DataFrame({"col1": ["Sun", "Sun", "Moon", "Earth", "Moon", "Venus"]})
df_new = pd.get_dummies(df, columns=["col1"], prefix="Planet")
print(df)

#     Planet_Earth  Planet_Moon  Planet_Sun  Planet_Venus
# 0             0            0           1             0                <== Sun
# 1             0            0           1             0                <== Sun
# 2             0            1           0             0                <== Moon
# 3             1            0           0             0                <== Earth
# 4             0            1           0             0                <== Moon
# 5             0            0           0             1                <== Venus
 ```

 See also [O], [Encoding], [One-Cold Encoding], [Ordinal Encoding]


# One-Shot Learning

 A [prompt engineering] technique to increase the accuracy of a [large language model].

 In addition to the task description, the model sees a single example of the task. No gradient updates are performed.

 ```
Translate English to French:                # Task description
sea otter => loutre de mer                  # Example
cheese =>                                   # Prompt
 ```

 ~ `not enough data --> encoder + similarity function?` Ex: I only have 1 picture of the employee, how can I recognize people coming in as an employee or not? Treat as a classification problem and each class correspond to 1 employee. How do you learn to train a classifier based on 1 sample? Better to look at this problem as a nearest-neighbor problem! Question: How do you detect similarities? Use a similarity function, an encoder! So sequence of (1) an encoder and (2) a nearest neighbor algorithm.

 ![]( {{site.assets}}/o/one_shot_learning.png ){: width="100%"}

 Few-, one-, and zero-shot settings are specialized cases of zero-shot task transfer. In a few-shot setting, the model is provided with a task description and as many examples as fit into the context window of the model. In a one-shot setting, the model is provided with exactly one example and, in a zero-shot setting, with no example.

 ![]( {{site.assets}}/o/one_shot_learning_accuracy.png ){: width="100%"}

 More at:
  * [https://en.wikipedia.org/wiki/One-shot_learning_(computer_vision)](https://en.wikipedia.org/wiki/One-shot_learning_(computer_vision))

 See also [O], [Encoder], [Few-Shot Learning], [Siamese Network], [Similarity Function], [Transfer Learning], [Zero-Shot Learning], [Zero-Shot Task Transfer]


# Online Learning

 ~ Learning from streamed data!

 In computer science, online learning is a method of machine learning in which data becomes available in a sequential order and is used to update the best predictor for future data at each step, as opposed to batch learning techniques which generate the best predictor by learning on the entire training data set at once. Online learning is a common technique used in areas of machine learning where it is computationally infeasible to train over the entire dataset, requiring the need of out-of-core algorithms. It is also used in situations where it is necessary for the algorithm to dynamically adapt to new patterns in the data, or when the data itself is generated as a function of time, e.g., stock price prediction. Online learning algorithms may be prone to catastrophic interference, a problem that can be addressed by incremental learning approaches.

 More at:
  * [https://ai.stackexchange.com/questions/10474/what-is-the-relation-between-online-or-offline-learning-and-on-policy-or-off](https://ai.stackexchange.com/questions/10474/what-is-the-relation-between-online-or-offline-learning-and-on-policy-or-off)
  * [https://en.wikipedia.org/wiki/Online_machine_learning](https://en.wikipedia.org/wiki/Online_machine_learning)

 See also [O], [Buffered Online Learning], [Offline Learning]


# Online Learning Algorithm

 An algorithm that supports [online learning].

 See also [O], ....


# Open Neural Network Exchange

# ONNX

 A common format to store a model?

 More at:
  * [https://onnx.ai/](https://onnx.ai/)

 See also [O], [OpenVINO Toolkit]


# OpenAI Company

 [Microsoft] + [OpenAI] ~ [Google] + [DeepMind]

 Models:
  * [ChatGPT][ChatGPT Model]: An fine-tuned model of GPT that is based on dialog
  * [CLIP][CLIP Model]: A model that can put a legend to an image
  * [Codex][Codex Model]: A [LLM] that specialize on generating code
  * [DALL-E][DALL-E Model]: A [Diffusion Model] that from text can generate images
  * [Five][OpenAI Five Model]: An agent that isnow world champion at the Dota2 game!
  * [GPT][GPT Model]: A Generative model for text
  * [Gym Environment][OpenAI Gym Environment]: Environments for development of [Reinforcement Learning] algorithms.
  * [Jukebox][Jukebox Model]: Generative model for music
  * [Whisper][Whisper Model]: A speech-to-text model

 Interfaces
  * [Triton] : GPU programming for neural networks

 People:
  * [Sam Altman][Sam Altman Person]: CEO
  * [Greg Brockman][Greg Brockman Person]: Co-founder
  * [Ilya Sutskever][Ilya Sutskever Person]: Co-founder
  * [Elon Mustk][Elon Musk Person]: Early investor
  * [Bill Gates][Bill Gates Person]: Early/Late investor

 More at:
  * home - [http://www.openai.com](http://www.openai.com)
  * principles - [https://openai.com/policies/usage-policies](https://openai.com/policies/usage-policies)
  * safety - [https://openai.com/safety-standards](https://openai.com/safety-standards)

 See also [O], [People]


# OpenAI Five Model

 September 2018
 5 agents (characters) that work together in collaboration! 
 In a game with incomplete information

 {% youtube "https://www.youtube.com/watch?v=tfb6aEUMC04" %}

 More at:
  * [https://openai.com/research/openai-five](https://openai.com/research/openai-five)
  * [https://openai.com/blog/openai-five-benchmark](https://openai.com/blog/openai-five-benchmark)
  * [https://www.twitch.tv/videos/293517383](https://www.twitch.tv/videos/293517383)
  * [https://openai.com/blog/openai-five-finals](https://openai.com/blog/openai-five-finals)
  * wikipedia - [https://en.wikipedia.org/wiki/OpenAI_Five](https://en.wikipedia.org/wiki/OpenAI_Five)

 See also [O], [AlphaStar Model]


# OpenAI Gym Environment

 Virtual environment built by [OpenAI] and to be used for [Reinforcement Learning] projects

 See also [O], [Rocket League Gym]


# OpenCV Module

 More at:
  * Sample project - [https://github.com/ecemay/open_cv_studies](https://github.com/ecemay/open_cv_studies)

 See also [O], [Computer Vision]


# OpenFold Model

 OpenFold is a non-profit AI research and development consortium developing free and open-source software tools for biology and drug discovery. Our mission is to bring the most powerful software ever created -- AI systems with the ability to engineer the molecules of life -- to everyone. These tools can be used by academics, biotech and pharmaceutical companies, or students learning to create the medicines of tomorrow, to accelerate basic biological research, and bring new cures to market that would be impossible to discover without AI.

 More at:
  * [https://openfold.io/](https://openfold.io/)

 See also [O], [AlphaFold Model], [ESM Metagenomic Atlas]


# OpenMMLab

 OpenMMLab team integrates open source, academic research, business application and system development, aiming at becoming a worldwide leader in open source algorithm platform for computer vision and providing advanced R&D models and systems for the industry.

 Based on [PyTorch]

 More at:
  * [https://openmmlab.com/](https://openmmlab.com/)

 See also [O], ...


# OpenVINO Toolkit
 
 More at:
  * home - [https://docs.openvino.ai/latest/home.html](https://docs.openvino.ai/latest/home.html)

 See also [O], [Open Neural Network Exchange]


# Optimization

 See also [O], [Dynamic Programming], [Loss Function], [Optimizer]


# Optimizer

 Optimizer = How you are going to minimize the [Loss Function].

 To minimize the prediction error or loss, the model while experiencing the examples of the training set, updates the model parameters W. These error calculations when plotted against the W is also called cost function plot J(w), since it determines the cost/penalty of the model. So minimizing the error is also called as minimization the cost function. But how exactly do you do that? Using optimizers.

 Optimization algorithm
  * The first algorithms
   * [Gradient Descent (GD)][GD Algorithm]
   * [Stochastic Gradient Descent (SGD)][SGD Algorithm] - faster convergence than GD
  * Adaptive learning algorithms
   * [Resilient Backpropagation (Rrop)][Rprop Algorithm] (1992)
   * [Adaptive Gradient Algorithm][AdaGrad Algorithm] 
   * [Root Mean Square Propagation (RMSprop)][RMSProp Algorithm] 
  * Momentum algorithms
   * [Gradient Descent with Momentum][GD with Momentum algorithm]
  * Adaptive learning with Momentum algorithms
   * [Adaptive Delta Algorithm][AdaDelta Algorithm] 
   * [Adaptive Momentum Estimation (Adam)][Adam Algorithm]
  * Others
   * Nesterov-accelerated Adaptive Momentum Estimation (Nadam)
   * Nadamax
   * Nesterov
   * MaxaProp

 
 ```
from tensorflow import keras
from tensorflow.keras import layers

model = keras.Sequential()
model.add(layers.Dense(64, kernel_initializer='uniform', input_shape=(10,)))
model.add(layers.Activation('softmax'))

opt = keras.optimizers.Adam(learning_rate=0.01)
model.compile(loss='categorical_crossentropy', optimizer=opt)
 ```

 More at:
  * [https://keras.io/api/optimizers/](https://keras.io/api/optimizers/)

 See also [O], ...


# Ordinal Encoding

 [Categorical features][Categorical Feature] often need to be converted to numeric values to facilitate processing by modeling algorithms.

 Ordinal encoding converts categorical column to numeric values when an ordered relationship exists.

 ![]( {{site.assets}}/o/ordinal_encoding.png )

 See also [O], [On-Cold Encoding], [One-Hot Encoding]


# Outlier

 Used in [Feature Engineering], ...
 THe opposite of [Inlier]

 Q: How to identify an outlier? --> [RANSAC Algorithm]

 See also [O], ...


# Output Layer

 See also [O], [Layer], [Artificial Neural Network]


# Output Perturbation

 We introduce gaussian noise in the output. Easy to implement as treat the model as a blackbox. But if people have access to the model itself, they can bypass noise the "addititor". In general, it is better if the noise is built in the model!

 See also [O], [Differential Privacy]


# Overfitting

 ~ a Model that is overly complex and leads to high [variance] and low [bias] = noise is memorized in model! . In contrast, a program that memorizes the training data by learning an overly-complex model could predict the values of the response variable for the training set accurately, but will fail to predict the value of the response variable for new examples.

 `A well functioning ML algorithm will separate the signal from the noise`.

 If the algorithm is too complex or flexible (e.g. it has too many input features or it‚Äôs not properly regularized), it can end up ‚Äúmemorizing the noise‚Äù instead of finding the signal. This overfit model will then make predictions based on that noise. It will perform unusually well on its training data‚Ä¶ yet very poorly on new, unseen data. `A key challenge with overfitting, and with machine learning in general, is that we can‚Äôt know how well our model will perform on new data until we actually test it`. To address this, we can split our initial [dataset] into separate [training][Training Subset] and [test subsets][Test Subset]. To avoid over-fitting, try [hyperparameter optimization].

 Reasons for overfitting:
  * too few training examples
  * running the training process for too many epochs. Consider [early stopping] ?

 ![]( {{site.assets}}/o/overfitting_fitting_comparison.png ){: width="100%"}

 Beware:
  * Overfitting is responsible for the [membership inference attack]
  * The opposite of overfitting is [underfitting]

 See also [O], [Balanced Fitting], [Bias], [Principal Component Analysis]


# Overtraining

 Overtraining is when a machine learning model can predict training examples with very high accuracy but cannot generalize to new data. This leads to poor performance in the field. Usually, this is a result of too little data or data that is too homogenous.

 See also [O], [Overfitting]
