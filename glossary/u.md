---
title: U
permalink: /u/

#=== THEMES
# = minima
layout: page
# = minimal mistake
# layout: archive-taxonomy
# layout: archive
# layout: categories
# layout: category
# layout: collection
# layout: compress
# layout: default
# layout: home
# layout: posts
# layout: search
## layout: single
# layout: splash
# layout: tag
# layout: tags

---

* toc
{:toc}

{% include links/all.md %}


# U-Net Architecture

 U-Net is considered as one of the standard CNN architectures for image classification tasks. It is considered as a best network for fast and precise segmentation of images. The UNET is an architecture which was developed by Olaf Ronneberger et al. for !BioMedical Image Segmentation. It mainly consists of two paths. One is an encoder path and other is a decoder path. The encoder path captures the context of the image producing feature maps. Encoder path is just a stack of convolution and max pooling layers. Decoder path used to enable precise localisation using transposed convolutions. U-net only contains Convolutional layers and does not contain any Dense layer because of which it can accept image of any size.

 ![]( {{site.assets}}/u/unet_architecture.png ){: width="100%"}

 See also [U], [Dense Layer], [Encoder-Decoder Model], [Instance Segmentation], [Semantic Segmentation], [U-Net Discriminator], [U-Net Generator]


# U-Net Discriminator

 See also [U], [U-Net Architecture]


# U-Net Generator

 The generator network used in AWS DeepComposer is adapted from the U-Net architecture, a popular convolutional neural network that is used extensively in the computer vision domain. The network consists of an “encoder” that maps the single track music data (represented as piano roll images) to a relatively lower dimensional “latent space“ and a ”decoder“ that maps the latent space back to multi-track music data. Here are the inputs provided to the generator:
  * Single-track piano roll: A single melody track is provided as the input to the generator.
  * Noise vector: A latent noise vector is also passed in as an input and this is responsible for ensuring that there is a flavor to each output generated by the generator, even when the same input is provided.

 ![]( {{site.assets}}/u/unet_generator.png ){: width="100%"}

 See also [U], [Convolutional Neural Network], [Encoder-Decoder Model], [Latent Space], [Noise Vector], [U-Net Architecture]


# Uncanny Valley

 When a robot is designed to look human, but is not quite there yet!

 See also [U], [Social Robot]


# Uniform Manifold Approximation and Projection

# UMAP

 {% youtube "https://www.youtube.com/watch?v=nq6iPZVUxZU" %}

 {% youtube "https://www.youtube.com/watch?v=eN0wFzBA4Sc" %}

 {% youtube "https://www.youtube.com/watch?v=jth4kEvJ3P8" %}

 See also [U], [Dimensionality Reduction]


# Underfitting

 When you have low variance (clustered), but high bias (offset). To prevent under-fitting try hyper-parameter optimization.

 ![]( {{site.assets}}/u/underfitting_overfitting_balanced.png ){: width="100%"}

 See also [U], [Balanced Fitting], [Hyperparameter Optimization], [Overfitting]


# Universal Function Approximator

 A [neural network] can approximate almost any function. The number of inputs need to be finite and each input should be able to be turned into numbers. Same for outputs.

 {% youtube "https://www.youtube.com/watch?v=0QczhVg5HaI" %}

 {% youtube "https://www.youtube.com/watch?v=Ijqkc7OLenI" %}

 More at:
  * playground - [https://playground.tensorflow.org/](https://playground.tensorflow.org/)

 See also [U], ...


# Unlabeled Data Algorithm

 See also [U], [Active Learning], [Data Augmentation], [Labeling Service], [Semi-Supervised Learning], [Snorkel Program], [Weak Supervision Labeling Function]


# Unstructured Data

 See also [U], [Data]


# Unsupervised Deep Learning Model

 See also [U], [Autoencoder], [Boltzmann Machine], [Self-Organizing Map], [Unsupervised Learning]


# Unsupervised Learning

 `No teacher, just observations and raw data!` ==> find clusters in the samples. Leaning like a toddler would! The computer learns by itself! No labeled data? No external teacher or pre-trained data. Model detects emerging properties in the input dataset. :warning: Dataset can be modified for training (e.g masked/MLM, etc). Model then constructs patterns or clusters. Further grouped into clustering and association. The machine tries to create label on its own. Best when the relationship between the input and the output is unknown (ex: new ways to do credit card fraud?) Example: clustering.

 Algorithms:
  * [Clustering] with 
    * [K-Means Clustering Algorithm]
    * [Self-Organizing Map (SOM)][SOM]
  * Association with [Apriori]

 More at:
  * ...

 See also [U], [Supervised Learning]


# Update Ratio

 A ratio of the number of times the discriminator is updated per generator training epoch. Updating the discriminator multiple times per generator training epoch is useful because it can improve the discriminators accuracy. Changing this ratio might allow the generator to learn more quickly early-on, but will increase the overall training time.

 See also [U], [Discriminator]


# Upstream Task

 The task executed in a pre-trained model.

 See also [U], [Pre-Trained Model], [Self-Supervised Learning]
